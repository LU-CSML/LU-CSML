<p>
<h2>Variational inference in Gaussian process models</h2>

Gaussian process models are widely used in statistics and machine learning. There are three key challenges to inference that might be tackled using variational methods: inference over the latent function values when the likelihood is non-Gaussian; scaling the computation to large datasets; inference over the kernel-parameters. I'll show how the variational framework can be used to tackle any or all of these challenges. In particular, I'll share recent insights which allow us to distinguish the variational stochastic process approximation, improving on the idea of a low-rank approximation to the posterior. To do this we show that it's possible to minimize the KL divergence between the true and approximate stochastic processes.
 
